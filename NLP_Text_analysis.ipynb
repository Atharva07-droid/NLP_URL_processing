{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import requests\n",
    "import openpyxl\n",
    "import os\n",
    "import string\n",
    "import nltk\n",
    "import syllables\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Input.csv')\n",
    "# contains URLS of wesbites and also the URL_NO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop through the URLs and save all data to files\n",
    "for url_id, url in zip(df['URL_ID'], df['URL']):\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    div_tag = soup.find('div', class_='td-post-content')\n",
    "\n",
    "    if div_tag != None:\n",
    "        data1 = div_tag.find_all(text=True)\n",
    "        data = ''\n",
    "        data = soup.find('title').text[:-23]\n",
    "\n",
    "        for text in data1:\n",
    "            data += (text)\n",
    "\n",
    "        f = open(str(url_id)+\".txt\", \"w+\", encoding=\"utf-8\")\n",
    "        n = f.write(data)\n",
    "        f.close()\n",
    "\n",
    "    else:\n",
    "        f = open(str(url_id)+\".txt\", \"w+\", encoding=\"utf-8\")\n",
    "        n = f.write(\"\")\n",
    "        f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for stop words in a folder\n",
    "# you can also use stop words from nltk library\n",
    "folder_path = 'StopWords'\n",
    "word_list = []\n",
    "\n",
    "for filename in os.listdir(folder_path):\n",
    "    file_path = os.path.join(folder_path, filename)\n",
    "\n",
    "    if os.path.isfile(file_path):\n",
    "        with open(file_path, 'r') as f:\n",
    "            text = f.read()\n",
    "\n",
    "        words = text.split()\n",
    "        word_list.extend(words)\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save output in the CSV file\n",
    "df_output = pd.read_csv('Output Data Structure.csv')\n",
    "print(df_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "\n",
    "positive_words = []\n",
    "\n",
    "with open(r\"enter you path for positve word files\", 'r', encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "f.close()\n",
    "positive = text.split()\n",
    "positive_words.extend(positive)\n",
    "\n",
    "# print(positive_words)\n",
    "\n",
    "negative_words = []\n",
    "\n",
    "with open(r\"enter you path for negative word files\", 'r') as f:\n",
    "    text = f.read()\n",
    "\n",
    "f.close()\n",
    "negative = text.split()\n",
    "negative_words.extend(negative)\n",
    "\n",
    "# print(negative_words)\n",
    "\n",
    "for url_id, url in zip(df_output['URL_ID'], df_output['URL']):\n",
    "    with open(str(url_id)+\".txt\", 'r+', encoding=\"utf-8\") as f:\n",
    "        text = f.read()\n",
    "\n",
    "    words = nltk.word_tokenize(text)\n",
    "    filtered_words = [word for word in words if word.lower() not in word_list]\n",
    "    positive_score = 0\n",
    "    negative_score = 0\n",
    "    for words in filtered_words:\n",
    "        if words in positive_words:\n",
    "            positive_score += 1\n",
    "        elif words in negative_words:\n",
    "            negative_score += -1\n",
    "\n",
    "    # calculate negative and it should be positive so *-1\n",
    "    negative_score *= -1\n",
    "\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    cleaned_sentences = []\n",
    "    for sentence in sentences:\n",
    "        words = nltk.word_tokenize(sentence)\n",
    "        filtered_words = [\n",
    "            word for word in words if word.lower() not in word_list]\n",
    "        cleaned_sentence = ' '.join(filtered_words)\n",
    "        cleaned_sentences.append(cleaned_sentence)\n",
    "    cleaned_sentences = ' '.join(cleaned_sentences)\n",
    "#     print(cleaned_sentences)\n",
    "\n",
    "#     print(positive_score)\n",
    "#     print(negative_score)\n",
    "    # calculate polarity score by formula\n",
    "    polarity_score = (positive_score - negative_score) / ((positive_score + negative_score) + 0.000001)\n",
    "#     print(polarity_score)\n",
    "\n",
    "    no_punct = cleaned_sentences.translate(\n",
    "        str.maketrans(\"\", \"\", string.punctuation))\n",
    "#     print(no_punct)\n",
    "\n",
    "    # calculate length of words in text\n",
    "    words = no_punct.split()\n",
    "    num_words = len(words)\n",
    "#     print(num_words)\n",
    "\n",
    "    subjectivity_score = (positive_score + negative_score) / \\\n",
    "        (num_words + 0.000001)\n",
    "#     print(subjectivity_score)\n",
    "\n",
    "    # calculate length of sentences\n",
    "    sentences = nltk.sent_tokenize(cleaned_sentences)\n",
    "    num_sentences = len(sentences)\n",
    "#     print(num_sentences)\n",
    "\n",
    "    # checking if data is empty or not in case the URL is invalid\n",
    "    if num_sentences > 0 and num_words > 0:\n",
    "        Average_Sentence_Length = num_words / num_sentences\n",
    "    #     print(Average_Sentence_Length)\n",
    "    else:\n",
    "        Average_Sentence_Length = 0\n",
    "\n",
    "    # checking for complex words\n",
    "    threshold = 3\n",
    "    words = cleaned_sentences.split()\n",
    "    num_complex_words = 0\n",
    "\n",
    "    for word in words:\n",
    "        syllable_count = syllables.estimate(word)\n",
    "        if syllable_count >= threshold:\n",
    "            num_complex_words += 1\n",
    "\n",
    "#     print(num_complex_words\n",
    "\n",
    "    if num_complex_words > 0 and num_words > 0:\n",
    "        Percentage_Complex_words = num_complex_words / num_words\n",
    "    #     print(Percentage_Complex_words)\n",
    "    else:\n",
    "        Percentage_Complex_words = 0\n",
    "\n",
    "    Fog_index = 0.4 * (Average_Sentence_Length + Percentage_Complex_words)\n",
    "#     print(Fog_index)\n",
    "\n",
    "    pronoun_count = re.compile(r'\\b(I|we|ours|my|mine|(?-i:us))\\b', re.I)\n",
    "    pronouns = pronoun_count.findall(cleaned_sentences)\n",
    "#     print(len(pronouns))\n",
    "\n",
    "    words = nltk.word_tokenize(no_punct)\n",
    "\n",
    "    total_length = sum(len(word) for word in words)\n",
    "\n",
    "    if total_length > 0 and len(words) > 0:\n",
    "        average_word_length = total_length / len(words)\n",
    "    else:\n",
    "        average_word_length = 0\n",
    "\n",
    "#     print(average_word_length)\n",
    "    df_output.loc[i, ['POSITIVE SCORE', 'NEGATIVE SCORE', 'POLARITY SCORE', 'SUBJECTIVITY SCORE', 'AVG SENTENCE LENGTH', 'PERCENTAGE OF COMPLEX WORDS', 'FOG INDEX', 'AVG NUMBER OF WORDS PER SENTENCE', 'COMPLEX WORD COUNT', 'WORD COUNT', 'SYLLABLE PER WORD', 'PERSONAL PRONOUNS',\n",
    "                      'AVG WORD LENGTH']] = [positive_score, negative_score, polarity_score, subjectivity_score, Average_Sentence_Length, Percentage_Complex_words, Fog_index, Average_Sentence_Length, num_complex_words, num_words, syllable_count, len(pronouns), average_word_length]\n",
    "    i += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking to see output is as desired\n",
    "print(df_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving all values to output CSV file\n",
    "df_output.to_csv('output.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7b866af7b652cc143acdbab46f35e39c45c92f19e64cafa2e94b399d464a3b2e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
